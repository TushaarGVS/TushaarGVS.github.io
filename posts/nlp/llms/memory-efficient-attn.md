---
layout: post
title: <a href="https://arxiv.org/pdf/2112.05682v3.pdf">Memory-efficient attention</a>
description: self-attention does not need O(n^2) memory
categories: [nlp, llms]
tags: [transformers, memory-efficient, attention]
date: 06/30/2023
next:
previous: performers
comments: true
output:
  html_document:
    highlight: pygments
  pdf_document: default
  word_document: default
---
{% include JB/setup %}

## Self-attention

{% highlight python %}
import numpy as np
import jax as jax
print('hello world')
{% endhighlight %}

```py
import numpy as np
import jax as jax
print('hello world')
```

## Memory-efficient attention

### Softmax trick

## References
