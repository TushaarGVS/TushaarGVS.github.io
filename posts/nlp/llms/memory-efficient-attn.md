---
layout: post
# change attributes between this comment [...]
title: <a href="https://arxiv.org/pdf/2112.05682v3.pdf">Memory-efficient attention</a>
description: self-attention does not need O(n^2) memory
categories: [nlp, llms]
tags: [transformers, memory-efficient, attention]
date: 2023-06-30
next:
previous: 'https://tushaargvs.github.io/posts/nlp/llms/performers.html'
# [...] and this comment. 
highlighter: rouge
kramdown:
  math_engine: mathjax
  syntax_highlighter: rouge
---
{% include JB/setup %}

### Self-attention

{% highlight python %}
import numpy as np
import jax as jax
print('hello world')
{% endhighlight %}

```python
import numpy as np
import jax as jax
print('hello world')
```

$L$ and $$L$$ and 

$$
A = bc
$$

### Memory-efficient attention

#### Softmax trick

### References
