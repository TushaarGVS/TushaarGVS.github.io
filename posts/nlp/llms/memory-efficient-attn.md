---
layout: post
title: <a href="https://arxiv.org/pdf/2112.05682v3.pdf">Memory-efficient attention</a>
description: self-attention does not need O(n^2) memory
categories: [nlp, llms]
tags: [transformers, memory-efficient, attention]
date: 2023-06-30
next:
previous: 'performers.html'
kramdown:
  math_engine: mathjax
  syntax_highlighter: rouge
---
{% include JB/setup %}

### Self-attention

{% highlight python %}
import numpy as np
import jax as jax
print('hello world')
{% endhighlight %}

### Memory-efficient attention

#### Softmax trick

### References
