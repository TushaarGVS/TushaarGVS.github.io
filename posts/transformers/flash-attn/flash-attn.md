---
layout: post
title: flash attention
tagline: (<a href="https://arxiv.org/abs/2205.14135">arxiv</a>)
description: Fast and Memory-Efficient Exact Attention with IO-Awareness
tags: [llms, transformers, flash, attention]
date: 2023-07-29
author: Tushaar Gangavarapu
next:
previous: 
---

### attention

### references

