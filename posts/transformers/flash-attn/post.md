---
layout: post
title: Flash attention
tagline: (<a href="https://arxiv.org/abs/2205.14135">arxiv</a>)
description: Fast and Memory-Efficient Exact Attention with IO-Awareness
tags: [llms, transformers, flash attention, self-attention, computational efficiency]
date: 2023-07-30
author: Tushaar Gangavarapu
next:
previous: 
---



### Preliminaries



### Footnotes

