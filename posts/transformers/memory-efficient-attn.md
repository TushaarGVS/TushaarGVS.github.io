---
layout: post
# change attributes between this comment [...]
title: <a href="https://arxiv.org/pdf/2112.05682v3.pdf">Memory-efficient attention</a>
description: self-attention does not need O(n^2) memory
categories: [nlp, llms]
tags: [transformers, memory-efficient, attention]
date: 2023-06-30
next:
previous: 
    url: 'posts/transformers/performers.html'
    title: performers
# [...] and this comment.
markdown: kramdown
highlighter: rouge
  input: GFM
kramdown:
  math_engine: mathjax
  syntax_highlighter: rouge
---
{% include JB/setup %}

### Self-attention

```python
import numpy as np
import jax as jax
print('hello world')
```

### Memory-efficient attention

#### Softmax trick

### References
