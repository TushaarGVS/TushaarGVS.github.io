---
layout: post
title: <a href="https://arxiv.org/pdf/2112.05682v3.pdf">Memory-efficient attention</a>
tagline: self-attention does not need $\mathcal{O}(n^2)$ memory
author: Tushaar Gangavarapu
categories: [nlp, llms]
tags: [transformers, memory-efficient, attention]
date: 2023-06-30
next:
previous: 
    url: 'posts/transformers/performers.html'
    title: performers
---
{% include JB/setup %}

### Self-attention

```python
import numpy as np
import jax as jax
print('hello world')
```

### Memory-efficient attention

#### Softmax trick

### References
