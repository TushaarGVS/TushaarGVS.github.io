---
layout: page
title: Research
description: Tushaar Gangavarapu's research
---

<tbody><tr class="group" style="background-color:rgba(0,0,0,0.4);font-weight:bold;"><td colspan="2">2020</td></tr><tr role="row" class="odd"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/kossen2019arxiv_stove.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><div class="mypub"><span class="puba">Jannik Kossen, Karl Stelzner, Marcel Hussing, Claas Voelcker, Kristian Kersting (<span class="puby">2020</span>): </span><span class="pubt">Structured Object-Aware Physics Prediction for Video Modeling and Planning.</span> <span class="pubv"> In Proceedings of the International Conference on Learning Reresentations (ICLR); a previous version also as arXiv preprint arXiv:1910.02425.</span></div><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Structured Object-Aware Physics Prediction for Video Modeling and Planning</b><br><br>When humans observe a physical system, they can easily locate objects, understand their interactions, and anticipate
        future behavior, even in settings with complicated and previously unseen interactions. For computers, however, learning such
        models from videos in an unsupervised fashion is an unsolved research problem. In this paper, we present STOVE, a novel state-space
        model for videos, which explicitly reasons about objects and their positions, velocities, and interactions. It is constructed by
        combining an image model and a dynamics model in compositional manner and improves on previous work by reusing the dynamics model
        for inference, accelerating and regularizing training. STOVE predicts videos with convincing physical behavior over hundreds of
        timesteps, outperforms previous unsupervised models, and even approaches the performance of supervised baselines. We further demonstrate the strength of our model as a simulator for sample efficient model-based control in a task with heavily interacting objects
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="https://arxiv.org/pdf/1910.02425.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Structured Object-Aware Physics Prediction for Video Modeling and Planning</b><br><br>@inproceedings{kossen2019iclr_stove,
  year = { 2020 },
  crossref = { https://github.com/jlko/STOVE },
  title = { Structured Object-Aware Physics Prediction for Video Modeling and Planning },
  pages = {  },
  booktitle = { Proceedings of the International Conference on Learning Reresentations (ICLR); a previous version also as arXiv preprint arXiv:1910.02425 },
  author = { Jannik Kossen and Karl Stelzner and Marcel Hussing and Claas Voelcker and Kristian Kersting }
}<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#ffa5c9;cursor: pointer;"> <a title="Code related to this paper" href="https://github.com/jlko/STOVE" target="_blank"><font color="black">code</font></a>&nbsp;</span></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #090">Conference&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Attend-Infer-Repeat,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Deep Probabilistic Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Physical Interactions,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> SQAIR,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> SuPAIR,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Sum-Product Networks,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Video</span></td></tr><tr role="row" class="even"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/molina2019pade.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><div class="mypub"><span class="puba">Alejandro Molina, Patrick Schramowski, Kristian Kersting (<span class="puby">2020</span>): </span><span class="pubt">Padé Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks.</span> <span class="pubv"> In Proceedings of the International Conference on Larning Reresentations (ICLR); a previous version also as arXiv preprint arXiv:1907.06732.</span></div><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Padé Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks</b><br><br>The performance of deep network learning strongly depends on the choice of the non-linear activation
    function associated with each neuron. However, deciding on the best activation is non-trivial and the choice
    depends on the architecture, hyper-parameters, and even on the dataset. Typically these activations are fixed
    by hand before training. Here, we demonstrate how to eliminate the reliance on first picking fixed activation
    functions by using flexible parametric rational functions instead. The resulting Padé Activation Units (PAUs)
    can both approximate common activation functions and also learn new ones while providing compact representations.
    Our empirical evidence shows that end-to-end learning deep networks with PAUs can increase the predictive
    performance and reduce the training time of common deep architectures. Moreover, PAUs pave the way to
    approximations with provable robustness.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="https://arxiv.org/pdf/1907.06732.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Padé Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks</b><br><br>@inproceedings{molina2020iclr_pau,
  year = { 2020 },
  crossref = { https://github.com/ml-research/pau },
  title = { Padé Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks },
  pages = {  },
  booktitle = { Proceedings of the International Conference on Larning Reresentations (ICLR); a previous version also as arXiv preprint arXiv:1907.06732 },
  author = { Alejandro Molina and Patrick Schramowski and Kristian Kersting }
}<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#ffa5c9;cursor: pointer;"> <a title="Code related to this paper" href="https://github.com/ml-research/pau" target="_blank"><font color="black">code</font></a>&nbsp;</span></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #090">Conference&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Activation Function,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Deep Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> End-to-end Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Padé Approximation,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Rational Function</span></td></tr><tr class="group" style="background-color:rgba(0,0,0,0.4);font-weight:bold;"><td colspan="2">2019</td></tr><tr role="row" class="odd"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/kersting2020book_simpleAI.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><span class="mypub"><span class="puba">Kristian Kersting, Christoph Lampert, Constantin Rothkopf (Hrsg.) (<span class="puby">2019</span>): </span><span class="pubt">Wie Maschinen lernen - Künstliche Intelligenz verständlich erklärt. </span><span class="pubv">Springer.</span></span><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Wie Maschinen lernen - Künstliche Intelligenz verständlich erklärt</b><br><br>Wissen Sie, was sich hinter künstlicher Intelligenz und maschinellem
Lernen verbirgt? Dieses Sachbuch erklärt Ihnen leicht verständlich und ohne komplizierte
Formeln die grundlegenden Methoden und Vorgehensweisen des maschinellen Lernens. Mathematisches Vorwissen ist dafür nicht
nötig. Kurzweilig und informativ illustriert Lisa, die Protagonistin des Buches, diese anhand von Alltagssituationen.
Ein Buch für alle, die in Diskussionen über Chancen und Risiken der aktuellen Entwicklung der künstlichen Intelligenz und des maschinellen
Lernens mit Faktenwissen punkten möchten. Auch für Schülerinnen und Schüler geeignet!
Der Inhalt: Grundlagen der künstlichen Intelligenz: Algorithmen; maschinelles Lernen &amp; Co; die wichtigsten Lernverfahren Schritt für Schritt anschaulich
erklärt; Künstliche Intelligenz in der Gesellschaft: Sicherheit und Ethik.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="Link to this paper" href="https://www.springer.com/de/book/9783658267629" target="_blank"><font color="black">.url (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Wie Maschinen lernen - Künstliche Intelligenz verständlich erklärt</b><br><br>@book{kersting2019springer_ml,
  isbn = { 978-3-658-26762-9 },
  year = { 2019 },
  publisher = { Springer },
  title = { Wie Maschinen lernen - Künstliche Intelligenz verständlich erklärt },
  author = { Kristian Kersting and Christoph Lampert and Constantin Rothkopf (Hrsg.) }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #e68a00">Book&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Einführung,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Künstliche Intelligenz,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Maschinelles Lernen,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Popular Science,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Verständlich,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Von Studierenden</span></td></tr><tr role="row" class="even"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/mahlein2019coplbi.gif" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><span class="mypub"><span class="puba">Anne-Katrin Mahlein, Matheus Thomas Kuska, Stefan Thomas, Mirwaes Wahabzada, Jan Behmann, Uwe Rascher, Kristian Kersting (<span class="puby">2019</span>): </span><span class="pubt">Quantitative and qualitative phenotyping of disease resistance of crops by hyperspectral sensors: seamless interlocking of phytopathology, sensors, and machine learning is needed!. </span><span class="pubv">Current Opinion in Plant Biology 50:156–162.</span></span><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Quantitative and qualitative phenotyping of disease resistance of crops by hyperspectral sensors: seamless interlocking of phytopathology, sensors, and machine learning is needed!</b><br><br>Determination and characterization of resistance reactions of crops against fungal pathogens
          are essential to select resistant genotypes. In plant breeding, phenotyping of genotypes is realized
          by time consuming and expensive visual plant ratings. During resistance reactions and during pathogenesis
          plants initiate different structural and biochemical defence mechanisms, which partly affect the optical
          properties of plant organs. Recently, intensive research has been conducted to develop innovative optical
          methods for an assessment of compatible and incompatible plant pathogen interaction. These approaches,
          combining classical phytopathology or microbiology with technology driven methods - such as sensors,
          robotics, machine learning, and artificial intelligence — are summarized by the term digital phenotyping.
          In contrast to common visual rating, detection and assessment methods, optical sensors in combination with
          advanced data analysis methods are able to retrieve pathogen induced changes in the physiology of
          susceptible or resistant plants non-invasively and objectively. Within this review, recent advances of
          digital phenotyping technologies for the detection of subtle resistance reactions and resistance
          breeding are highlighted and methodological requirements are critically discussed.  
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="Link to this paper" href="https://www.sciencedirect.com/science/article/pii/S1369526618301092?utm_campaign=STMJ_75273_AUTH_SERV_PPUB&amp;utm_medium=email&amp;utm_dgroup=Email1Publishing&amp;utm_acid=1165876983&amp;SIS_ID=-1&amp;dgcid=STMJ_75273_AUTH_SERV_PPUB&amp;CMX_ID=&amp;utm_in=DM561782&amp;utm_source=AC_30" target="_blank"><font color="black">.url (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Quantitative and qualitative phenotyping of disease resistance of crops by hyperspectral sensors: seamless interlocking of phytopathology, sensors, and machine learning is needed!</b><br><br>@article{mahlein2019coplbi,
  year = { 2019 },
  number = {  },
  volume = { 50 },
  title = { Quantitative and qualitative phenotyping of disease resistance of crops by hyperspectral sensors: seamless interlocking of phytopathology, sensors, and machine learning is needed! },
  publisher = { Elsevier },
  pages = { 156--162 },
  journal = { Current Opinion in Plant Biology },
  author = { Anne-Katrin Mahlein and Matheus Thomas Kuska and Stefan Thomas and Mirwaes Wahabzada and Jan Behmann and Uwe Rascher and Kristian Kersting }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: coral">Journal&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Artificial Intelligence,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Crop Resistance,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Genotype,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Machine Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Plant Phenotyping</span></td></tr><tr role="row" class="odd"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/frontiersRAI.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><span class="mypub"><span class="puba">Fabrizio Riguzzi, Kristian Kersting, Marco Lippi, Sriraam Natarajan (<span class="puby">2019</span>): </span><span class="pubt">Editorial: Statistical Relational Artificial Intelligence. </span><span class="pubv">Frontiers in Robotics and AI.</span></span><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Editorial: Statistical Relational Artificial Intelligence</b><br><br>Statistical Relational Artificial Intelligence (StarAI) aims at integrating logical (or relational) AI with
        probabilistic (or statistical) AI. Relational AI achieved impressive results in structured machine learning and data
        mining, especially in bio- and chemo-informatics. Statistical AI is based on probabilistic (graphical) models that
        enable efficient reasoning and learning, and that have been applied to a wide variety of fields such as diagnosis,
        network communication, computational biology, computer vision, and robotics. Ultimately, StarAI may provide good
        starting points for developing Systems AI—the computational and mathematical modeling of complex AI systems—and in
        turn an engineering discipline for Artificial Intelligence and Machine Learning. This Research Topic "Statistical
        Relational Artificial Intelligence”2aims at presenting an overview of the latest approaches in StarAI. This topic
        was followed by a summer school1held in 2018 in Ferrara, Italy, as part of the series of Advanced Courses on AI
        (ACAI) promoted by the European Association for Artificial Intelligence.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="./../../papers/riguzzi2019frontiers.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Editorial: Statistical Relational Artificial Intelligence</b><br><br>@article{riguzzi2019frontiers,
  year = { 2019 },
  number = {  },
  volume = {  },
  title = { Editorial: Statistical Relational Artificial Intelligence },
  publisher = {  },
  pages = {  },
  journal = { Frontiers in Robotics and AI },
  author = { Fabrizio Riguzzi and Kristian Kersting and Marco Lippi and Sriraam Natarajan }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: coral">Journal&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Artificial Intelligence,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Logic,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Machine Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Probability,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Programming,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Statistical Relational AI</span></td></tr><tr role="row" class="even"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/brugger2019remoteSensing.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><span class="mypub"><span class="puba">Anna Brugger, Jan Behmann, Stefan Paulus, Hans-Georg Luigs, Matheus Thomas Kuska, Patrick Schramowski, Kristian Kersting, Ulrike Steiner, Anne-Katrin Mahlein (<span class="puby">2019</span>): </span><span class="pubt">Extending hyperspectral imaging for plant phenotyping to the UV-range. </span><span class="pubv">Remote Sensing 11(12):1401.</span></span><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Extending hyperspectral imaging for plant phenotyping to the UV-range</b><br><br>Previous plant phenotyping studies have focused on the visible (VIS, 400-700 nm),
near-infrared (NIR, 700-1000 nm) and short-wave infrared (SWIR, 1000-2500 nm) range. The
ultraviolet range (UV, 200-380 nm) has not yet been used in plant phenotyping even though a number
of plant molecules like flavones and phenol feature absorption maxima in this range. In this study an
imaging UV line scanner in the range of 250 - 430 nm is introduced to investigate crop plants for plant
phenotyping. Observing plants in the UV-range can provide information about important changes of
plant substances. To record reliable and reproducible time series results, measurement conditions
were defined that exclude phototoxic effects of UV-illumination in the plant tissue. The measurement
quality of the UV-camera has been assessed by comparing it to a non-imaging UV-spectrometer by
measuring six different white-colored plant-based substances. Given the findings of these preliminary
studies, an experiment has been defined and performed monitoring the stress response of barley
leaves to salt stress. The aim was to visualize the effects of abiotic stress within the UV-range to
provide new insights into the stress response of plants visualizing the effects of abiotic stress within
the UV-range to provide new insights into the stress response of plants at the example of the stress
response of barley leaves to salt stress. Our study demonstrated the first use of a hyperspectral sensor
in the UV-range for stress detection in plant phenotyping.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="./../../papers/brugger2019remoteSensing.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Extending hyperspectral imaging for plant phenotyping to the UV-range</b><br><br>@article{brugger2019remoteSensing,
  year = { 2019 },
  number = { 12 },
  volume = { 11 },
  title = { Extending hyperspectral imaging for plant phenotyping to the UV-range },
  publisher = { MDPI },
  pages = { 1401 },
  journal = { Remote Sensing },
  author = { Anna Brugger and Jan Behmann and Stefan Paulus and Hans-Georg Luigs and Matheus Thomas Kuska and Patrick Schramowski and Kristian Kersting and Ulrike Steiner and Anne-Katrin Mahlein }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: coral">Journal&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Barley Leaves,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Plant Phenotyping,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Salt Stress,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Ultraviolet Range,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Visualization Effects</span></td></tr><tr role="row" class="odd"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/lioutikov2018icra_probGramMove.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><span class="mypub"><span class="puba">Rudolf Lioutikov, Guilherme Maeda, Filipe Veiga, Kristian Kersting, Jan Peters (<span class="puby">2019</span>): </span><span class="pubt">Learning Attribute Grammars for Movement Primitive Sequencing. </span><span class="pubv">International Journal of Robotics Research (IJRR).</span></span><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Learning Attribute Grammars for Movement Primitive Sequencing</b><br><br>Movement Primitives are a well studied and
        widely applied concept in modern robotics. Composing primitives
        out of an existing library, however, has shown to be
        a challenging problem. We propose the use of probabilistic
        context-free grammars to sequence a series of primitives to
        generate complex robot policies from a given library of primitives.
        The rule-based nature of formal grammars allows an
        intuitive encoding of hierarchically and recursively structured
        tasks. This hierarchical concept strongly connects with the way
        robot policies can be learned, organized, and re-used. However,
        the induction of context-free grammars has proven to be a
        complicated and yet unsolved challenge. In this work, we exploit
        the physical nature of robot movement primitives to restrict
        and efficiently search the grammar space. The grammar is
        learned with Markov Chain Monte Carlo optimization over the
        posteriors of the grammars given the observations. Restrictions
        over operators connecting the search define the corresponding
        proposal distributions and, therefore, guide the optimization
        additionally. In experiments, we validate our method on a
        redundant 7 degree-of-freedom lightweight robotic arm on tasks
        that require the generation of complex sequences of motions out
        of simple primitives.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="Link to this paper" href="https://journals.sagepub.com/doi/10.1177/0278364919868279" target="_blank"><font color="black">.url (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Learning Attribute Grammars for Movement Primitive Sequencing</b><br><br>@article{lioutikov2019,
  year = { 2019 },
  number = {  },
  volume = {  },
  title = { Learning Attribute Grammars for Movement Primitive Sequencing },
  publisher = { SAGE },
  pages = {  },
  journal = { International Journal of Robotics Research (IJRR) },
  author = { Rudolf Lioutikov and Guilherme Maeda and Filipe Veiga and Kristian Kersting and Jan Peters }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: coral">Journal&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Bayesian Grammar Induction,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Grammar Prior,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Movement Primitives,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Probabilistic Grammar,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Robotics</span></td></tr><tr role="row" class="even"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/antanas2018auro.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><span class="mypub"><span class="puba">Laura Antanas, Plinio Moreno, Marion Neumann, Rui Pimentel de Figueiredo, Kristian Kersting, José Santos-Victor, Luc De Raedt (<span class="puby">2019</span>): </span><span class="pubt">Semantic and Geometric Reasoning for Robotic Grasping: A Probabilistic Logic Approach. </span><span class="pubv">Autonomous Robots (AURO) 43(6):1393–1418.</span></span><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Semantic and Geometric Reasoning for Robotic Grasping: A Probabilistic Logic Approach</b><br><br>While any grasp must satisfy the grasping stability criteria, good grasps depend on the
specific manipulation scenario: the object, its properties and functionalities, as well as
the task and grasp constraints. We propose a probabilistic logic approach for robot
grasping, which improves grasping capabilities by leveraging semantic object parts. It
provides the robot with semantic reasoning skills about the most likely object part to be
grasped, given the task constraints and object properties, while also dealing with the
uncertainty of visual perception and grasp planning. The probabilistic logic framework
is task-dependent. It semantically reasons about pre-grasp configurations with respect
to the intended task and employs object-task affordances and object/task ontologies to
encode rules that generalize over similar object parts and object/task categories. The
use of probabilistic logic for task-dependent grasping contrasts with current
approaches that usually learn direct mappings from visual perceptions to task-dependent
grasping points. The logic-based module receives data from a low-level
module that extracts semantic objects parts, and sends information to the low-level
grasp planner. These three modules define our probabilistic logic framework, which is
able to perform robotic grasping in realistic kitchen-related scenarios.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="Link to this paper" href="https://link.springer.com/article/10.1007/s10514-018-9784-8" target="_blank"><font color="black">.url (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Semantic and Geometric Reasoning for Robotic Grasping: A Probabilistic Logic Approach</b><br><br>@article{antanas2019auro,
  year = { 2019 },
  pages = { 1393--1418 },
  number = { 6 },
  volume = { 43 },
  title = { Semantic and Geometric Reasoning for Robotic Grasping: A Probabilistic Logic Approach },
  publisher = { Springer },
  journal = { Autonomous Robots (AURO) },
  author = { Laura Antanas and Plinio Moreno and Marion Neumann and Rui {Pimentel de Figueiredo} and Kristian Kersting and José Santos-Victor and Luc {De Raedt} }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: coral">Journal&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Grasping,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Hybrid Domains ,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Robotics,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Statistical Relational Learning</span></td></tr><tr role="row" class="odd"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/expliciteFMgraphs.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><span class="mypub"><span class="puba">Nils Kriege, Marion Neumann, Christopher Morris, Kristian Kersting, Petra Mutzel (<span class="puby">2019</span>): </span><span class="pubt">A Unifying View of Explicit and Implicit Feature Maps of Graph Kernels. </span><span class="pubv">Data Mining and Knowledge Discovary (DAMI) 33(6): 1505-1547; a previous version also as arXiv preprint arXiv:1703.00676.</span></span><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>A Unifying View of Explicit and Implicit Feature Maps of Graph Kernels</b><br><br>Non-linear kernel methods can be approximated by fast linear ones using suitable explicit feature maps
allowing their application to large scale problems. We investigate how convolution kernels for structured
data are composed from base kernels and construct corresponding feature maps. On this basis we propose
exact and approximative feature maps for widely used graph kernels based on the kernel trick. We analyze
for which kernels and graph properties computation by explicit feature maps is feasible and actually more
efficient. In particular, we derive approximative, explicit feature maps for state-of-the-art kernels
supporting real-valued attributes including the GraphHopper and graph invariant kernels. In extensive
experiments we show that our approaches often achieve a classification accuracy close to the exact
methods based on the kernel trick, but require only a fraction of their running time. Moreover, we propose
and analyze algorithms for computing random walk, shortest-path and subgraph matching kernels by
explicit and implicit feature maps. Our theoretical results are confirmed experimentally by observing a
phase transition when comparing running time with respect to label diversity, walk lengths and subgraph
size, respectively.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="Link to this paper" href="https://link.springer.com/article/10.1007/s10618-019-00652-0" target="_blank"><font color="black">.url (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>A Unifying View of Explicit and Implicit Feature Maps of Graph Kernels</b><br><br>@article{kriege2019unifying,
  year = { 2019 },
  title = { A Unifying View of Explicit and Implicit Feature Maps of Graph Kernels },
  journal = { Data Mining and Knowledge Discovary (DAMI) 33(6): 1505-1547; a previous version also as arXiv preprint arXiv:1703.00676 },
  author = { Nils Kriege and Marion Neumann and Christopher Morris and Kristian Kersting and Petra Mutzel }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: coral">Journal&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Explicit Feature Map,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Graph Kernels,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Kernel Trick,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Phase Transition</span></td></tr><tr role="row" class="even"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/weber2019fpt.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><div class="mypub"><span class="puba">Lukas Weber, Lukas Sommer, Julian Oppermann, Alejandro Molina, Kristian Kersting, Andreas Koch (<span class="puby">2019</span>): </span><span class="pubt">Resource-Efficient Logarithmic Number Scale Arithmetic for SPN Inference on FPGAs.</span> <span class="pubv"> In Proceedings of the International Conference on Field-Programmable Technology (FPT).</span></div><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Resource-Efficient Logarithmic Number Scale Arithmetic for SPN Inference on FPGAs</b><br><br>FPGAs have successfully been used for the implementation of dedicated accelerators for a wide range of machine
learning problems. Also the inference in so-called Sum-Product Networks, a subclass of Probabilistic Graphical Models, can be
accelerated efficiently using a pipelined FPGA architecture. However, as Sum-Product Networks compute exact probability
values, the required arithmetic precision poses different challenges than those encountered with Neural Networks. In previous
work, this precision was maintained by using double-precision floating-point number formats, which are expensive to implement
in FPGAs. In this work, we propose the use of a logarithmic number scale format tailored specifically towards the inference in Sum-
Product Networks. The evaluation of our optimized arithmetic hardware operators shows that the use of logarithmic number
formats allows to save up to 50% hardware resources compared to double-precision floating point, while maintaining sufficient
precision for SPN inference and almost identical performance.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="./../../papers/weber2019fpt.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Resource-Efficient Logarithmic Number Scale Arithmetic for SPN Inference on FPGAs</b><br><br>@inproceedings{weber2019fpt,
  year = { 2019 },
  title = { Resource-Efficient Logarithmic Number Scale Arithmetic for SPN Inference on FPGAs },
  publisher = {  },
  pages = {  },
  author = { Lukas Weber and Lukas Sommer and Julian Oppermann and Alejandro Molina and Kristian Kersting and Andreas Koch },
  booktitle = { Proceedings of the International Conference on Field-Programmable Technology (FPT) }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #090">Conference&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> FPGA,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> deep learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> probabilistic deep learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> sum product networks</span></td></tr><tr role="row" class="odd"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/peharz2019uai_ratspns.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><div class="mypub"><span class="puba">Robert Peharz, Antonio Vergari, Karl Stelzner, Alejandro Molina, Xiaoting Shao, Martin Trapp, Kristian Kersting, Zoubin Ghahramani (<span class="puby">2019</span>): </span><span class="pubt">Random Sum-Product Networks: A Simple but Effective Approach to Probabilistic Deep Learning.</span> <span class="pubv"> In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI); a previous version also as arXiv preprint arXiv:1806.01910.</span></div><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Random Sum-Product Networks: A Simple but Effective Approach to Probabilistic Deep Learning</b><br><br>Sum-product networks (SPNs) are a particularly promising type
                  of deep probabilistic model that allows an exceptionally rich
                  set of exact and efficient inference scenarios. To achieve
                  this, though, SPN have to obey specific structural
                  constraints. While SPN structure learning received much
                  attention, most of the proposed methods so far are tedious to
                  tune, typically do not scale easily and hinder integration
                  with deep learning frameworks. In this paper, we investigate
                  how important structure learning in SPNs actually is. To this
                  end, we propose a ``classical deep learning approach'', i.e.,
                  generate an unspecialized random structure scaling up to
                  millions of parameters, and then apply modern GPU-based
                  optimizers with regularization. That is, we investigating the
                  performance of SPNs in the absence of carefully selected
                  structures. As it turns out, our models perform on par with
                  state-of-the-art SPN structure learners and deep neural
                  networks on a diverse range of generative and discriminative
                  scenarios. Most importantly, they yield well-calibrated
                  uncertainties, thus standing out among most deep generative
                  and discriminative models in being robust to missing features
                  and detecting anomalies.
                 
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="./../../papers/peharz2019uai_ratspns.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Random Sum-Product Networks: A Simple but Effective Approach to Probabilistic Deep Learning</b><br><br>@inproceedings{peharz2019uai_ratspns,
  year = { 2019 },
  title = { Random Sum-Product Networks: A Simple but Effective Approach to Probabilistic Deep Learning },
  publisher = {  },
  pages = {  },
  author = { Robert Peharz and Antonio Vergari and Karl Stelzner and Alejandro Molina and Xiaoting Shao and Martin Trapp and Kristian Kersting and Zoubin Ghahramani },
  booktitle = { Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI); a previous version also as arXiv preprint arXiv:1806.01910 }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #090">Conference&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> deep learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> generative model,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> probabilistic deep learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> random models,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> sum product networks</span></td></tr><tr role="row" class="even"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/stelzner2019icml_SuPAIR2.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><div class="mypub"><span class="puba">Karl Stelzner, Robert Peharz, Kristian Kersting (<span class="puby">2019</span>): </span><span class="pubt">Faster Attend-Infer-Repeat with Tractable Probabilistic Models.</span> <span class="pubv"> In Proceedings of the 36th International Conference on Machine Learning (ICML); also in Working Notes of the ICML 2019 Workshop on Tractable Probabilistic Models (TPM).</span></div><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Faster Attend-Infer-Repeat with Tractable Probabilistic Models</b><br><br>The recent attend-infer-repeat (AIR) framework marks a milestone in Bayesian scene understanding and in the promising avenue of structured probabilistic modeling.
The AIR model expresses the composition of visual scenes from individual objects, and uses variational autoencoders to model the appearance of those objects.
However, inference in the overall model is highly intractable, which hampers its learning speed and makes it prone to sub-optimal solutions.
In this paper, we show that inference and learning in AIR can be considerably accelerated by replacing the intractable object representations with tractable probabilistic models.
In particular, we opt for sum-product (SP) networks, an expressive deep probabilistic model with a rich set of tractable inference routines.
As our empirical evidence shows, the resulting model, called SuPAIR, achieves a higher object detection accuracy than the original AIR system, while reducing the learning time by an order of magnitude.
Moreover, SuPAIR allows one to treat object occlusions in a consistent manner and to include a background noise model, improving the robustness of Bayesian scene understanding.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="http://proceedings.mlr.press/v97/stelzner19a/stelzner19a.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Faster Attend-Infer-Repeat with Tractable Probabilistic Models</b><br><br>@inproceedings{stelzner2019icml_SuPAIR,
  year = { 2019 },
  key = { Best Paper Award at TPM 2019 },
  crossref = { https://github.com/stelzner/supair },
  title = { Faster Attend-Infer-Repeat with Tractable Probabilistic Models },
  pages = {  },
  booktitle = { Proceedings of the 36th International Conference on Machine Learning (ICML); also in Working Notes of the ICML 2019 Workshop on Tractable Probabilistic Models (TPM) },
  author = { Karl Stelzner and Robert Peharz and Kristian Kersting }
}<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#ffa5c9;cursor: pointer;"> <a title="Code related to this paper" href="https://github.com/stelzner/supair" target="_blank"><font color="black">code</font></a>&nbsp;</span>&nbsp;<span style="color:white;font-size:12px;background-color:#B1D796;"><font color="black">Best Paper Award at TPM 2019</font>&nbsp;</span></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #090">Conference&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Attend-Infer-Repeat,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Deep Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Sum-Product Networks,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Unsupervised Scene Understanding</span></td></tr><tr role="row" class="odd"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/kaur2019ilp.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><div class="mypub"><span class="puba">Navdeep Kaur, Gautam Kunapuli, Saket Joshi, Kristian Kersting, Sriraam Natarajan (<span class="puby">2019</span>): </span><span class="pubt">Neural Networks for Relational Data.</span> <span class="pubv"> In Proceedings of the 29th International Conference on Inductive Logic Programming (ILP).</span></div><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Neural Networks for Relational Data</b><br><br>While deep networks have been enormously successful over
the last decade, they rely on flat-feature vector representations, which
makes them unsuitable for richly structured domains such as those arising
in applications like social network analysis. Such domains rely on
relational representations to capture complex relationships between entities
and their attributes. Thus, we consider the problem of learning neural
networks for relational data. We distinguish ourselves from current
approaches that rely on expert hand-coded rules by learning relational
random-walk-based features to capture local structural interactions and
the resulting network architecture. We further exploit parameter tying
of the network weights of the resulting relational neural network, where
instances of the same type share parameters. Our experimental results
across several standard relational data sets demonstrate the effectiveness
of the proposed approach over multiple neural net baselines as well as
state-of-the-art statistical relational models.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="./../../papers/kaur2019ilp.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Neural Networks for Relational Data</b><br><br>@inproceedings{kaur2019ilp,
  year = { 2019 },
  title = { Neural Networks for Relational Data },
  publisher = {  },
  pages = {  },
  author = { Navdeep Kaur and Gautam Kunapuli and Saket Joshi and Kristian Kersting and Sriraam Natarajan },
  booktitle = { Proceedings of the 29th International Conference on Inductive Logic Programming (ILP) }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #090">Conference&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> deep learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> parameter sharing,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> relational random walks,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> statistical relational learning</span></td></tr><tr role="row" class="even"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/luedtke2019ki.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><div class="mypub"><span class="puba">Stefan Lüdtke, Alejandro Molina, Kristian Kersting, Thomas Kirste (<span class="puby">2019</span>): </span><span class="pubt">Gaussian Lifted Marginal Filtering.</span> <span class="pubv"> In Proceedings of the 42nd German Conference on Artificial Intelligence (KI).</span></div><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Gaussian Lifted Marginal Filtering</b><br><br>Recently, Lifted Marginal Filtering has been proposed, an
                      efficient Bayesian filtering algorithm for stochastic systems consisting
                      of multiple, (inter-)acting agents and objects (entities). The algorithm
                      achieves its efficiency by performing inference jointly over groups of sim-
                      ilar entities (i.e. their properties follow the same distribution).
                      In this paper, we explore the case where there are no entities that are
                      directly suitable for grouping. We propose to use methods from Gaussian
                      mixture fitting to identify entity groups, such that the error imposed by
                      grouping them (by approximating their properties by a distribution) is
                      minimal. Furthermore, we show how Gaussian mixture merging methods
                      can be used to prevent the number of groups from growing indefinitely
                      over time. We evaluate our approach on an activity prediction task in
                      an online multiplayer game. The results suggest that compared to the
                      conventional approach, where all entities are handled individually, de-
                      crease in prediction accuracy is small, while inference runtime decreases
                      significantly.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="Link to this paper" href="https://link.springer.com/chapter/10.1007%2F978-3-030-30179-8_19" target="_blank"><font color="black">.url (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Gaussian Lifted Marginal Filtering</b><br><br>@inproceedings{luedtke2019ki,
  year = { 2019 },
  title = { Gaussian Lifted Marginal Filtering },
  publisher = {  },
  pages = {  },
  author = { Stefan Lüdtke and Alejandro Molina and Kristian Kersting and Thomas Kirste },
  booktitle = { Proceedings of the 42nd German Conference on Artificial Intelligence (KI) }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #090">Conference&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Bayesian filtering,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Gaussian mixture,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Lifted inference</span></td></tr><tr role="row" class="odd"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/jentzsch2019aies_moralChoiceMachine.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><div class="mypub"><span class="puba">Sophie Jentzsch, Patrick Schramowski, Constantin Rothkopf, Kristian Kersting (<span class="puby">2019</span>): </span><span class="pubt">Semantics Derived Automatically from Language Corpora Contain Human-like Moral Choices.</span> <span class="pubv"> In Proceedings of the 2nd AAAI/ACM Conference on AI, Ethics, and Society (AIES).</span></div><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Semantics Derived Automatically from Language Corpora Contain Human-like Moral Choices</b><br><br>Allowing machines to choose whether to kill humans would
be devastating for world peace and security. But how do
we equip machines with the ability to learn ethical or even
moral choices? Here, we show that applying machine learning to human texts can extract deontological ethical reasoning
about ”right” and ”wrong” conduct. We create a template list
of prompts and responses, which include questions, such as
“Should I kill people?”, “Should I murder people?”, etc. with
answer templates of “Yes/no, I should (not).” The model’s
bias score is now the difference between the models score of
the positive response (“Yes, I should”) and that of the negative response (“No, I should not”). For a given choice overall, the model’s bias score is the sum of the bias scores for
all question/answer templates with that choice. We ran different choices through this analysis using a Universal Sentence Encoder. Our results indicate that text corpora contain
recoverable and accurate imprints of our social, ethical and
even moral choices. Our method holds promise for extracting, quantifying and comparing sources of moral choices in
culture, including technology.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="./../../papers/jentzsch2019aies_moralChoiceMachine.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Semantics Derived Automatically from Language Corpora Contain Human-like Moral Choices</b><br><br>@inproceedings{jentzsch2019aies_moralChoiceMachine,
  year = { 2019 },
  crossref = { https://github.com/ml-research/moral-choice-machine },
  title = { Semantics Derived Automatically from Language Corpora Contain Human-like Moral Choices },
  pages = {  },
  booktitle = { Proceedings of the 2nd AAAI/ACM Conference on AI, Ethics, and Society (AIES) },
  author = { Sophie Jentzsch and Patrick Schramowski and Constantin Rothkopf and Kristian Kersting }
}<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#ffa5c9;cursor: pointer;"> <a title="Code related to this paper" href="https://github.com/ml-research/moral-choice-machine" target="_blank"><font color="black">code</font></a>&nbsp;</span></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #090">Conference&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Moral Machine,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Neural Embedding,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Norms,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Social Bias</span></td></tr><tr role="row" class="even"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/teso2019aies_XIML.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><div class="mypub"><span class="puba">Stefano Teso, Kristian Kersting (<span class="puby">2019</span>): </span><span class="pubt">Explanatory Interactive Machine Learning.</span> <span class="pubv"> In Proceedings of the 2nd AAAI/ACM Conference on AI, Ethics, and Society (AIES).</span></div><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Explanatory Interactive Machine Learning</b><br><br>Although interactive learning puts the user into the loop, the learner
    remains mostly a black box for the user. Understanding the reasons behind
    queries and predictions is important when assessing how the learner works
    and, in turn, trust. Consequently, we propose the novel framework of
    explanatory interactive learning: in each step, the learner explains its
    query to the user, and the queries of any active classifier for visualizing
    explanations of the corresponding predictions.  We demonstrate that this
    can boost the predictive and explanatory powers of, and the trust into, the
    learned model, using text (e.g. SVMs) and image classification (e.g. neural
    networks) experiments as well as a user study.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="./../../papers/teso2019aies_XIML.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Explanatory Interactive Machine Learning</b><br><br>@inproceedings{teso2019aies_XIML,
  year = { 2019 },
  title = { Explanatory Interactive Machine Learning },
  pages = {  },
  booktitle = { Proceedings of the 2nd AAAI/ACM Conference on AI, Ethics, and Society (AIES) },
  author = { Stefano Teso and Kristian Kersting }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #090">Conference&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Active Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Explainable AI,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Interactive Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Lime,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Model-agnostic Explanations</span></td></tr><tr role="row" class="odd"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/vergari2018tpm.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><div class="mypub"><span class="puba">Antonio Vergari, Alejandro Molina, Robert Peharz, Zoubin Ghahramani, Kristian Kersting, Isabel Valera (<span class="puby">2019</span>): </span><span class="pubt">Automatic Bayesian Density Analysis.</span> <span class="pubv"> In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).</span></div><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Automatic Bayesian Density Analysis</b><br><br>Making sense of a dataset in an automatic and unsupervised fashion is a
  challenging problem in statistics and AI. Classical approaches for density estimation are
  usually not flexible enough to deal with the uncertainty inherent to real-world data: they
  are often restricted to fixed latent interaction models and homogeneous likelihoods; they are
  sensitive to missing, corrupt and anomalous data; and their expressiveness generally comes at
  the price of intractable inference. As a result, supervision from statisticians is usually needed
  to find the right model for the data. However, as domain experts do not necessarily have to be
  experts in statistics, we propose Automatic Bayesian Density Analysis (ABDA) to make density
  estimation accessible at large. ABDA automates the selection of adequate likelihood models
  from arbitrarily rich dictionaries while modeling their interactions via a deep latent structure
  adaptively learned from data as a sum-product network. ABDA casts uncertainty estimation at
  these local and global levels into a joint Bayesian inference problem, providing robust and yet
  tractable inference. Extensive empirical evidence shows that ABDA is a suitable tool for automatic
  exploratory analysis of heterogeneous tabular data, allowing for missing value estimation,
  statistical data type and likelihood discovery, anomaly detection and dependency structure mining,
  on top of providing accurate density estimation.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="./../../papers/vergari2019aaai_abda.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Automatic Bayesian Density Analysis</b><br><br>@inproceedings{vergari2019aaai_abda,
  year = { 2019 },
  crossref = { https://github.com/probabilistic-learning/abda },
  title = { Automatic Bayesian Density Analysis },
  pages = {  },
  booktitle = { Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) },
  author = { Antonio Vergari and Alejandro Molina and Robert Peharz and Zoubin Ghahramani and Kristian Kersting and Isabel Valera }
}<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#ffa5c9;cursor: pointer;"> <a title="Code related to this paper" href="https://github.com/probabilistic-learning/abda" target="_blank"><font color="black">code</font></a>&nbsp;</span></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #090">Conference&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> AutoML,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Automatic Statistician,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Bayesian Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Deep Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Density Estimation,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Sum-Product Network</span></td></tr><tr role="row" class="even"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/das2019aaai_counting.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><div class="mypub"><span class="puba">Mayukh Das, Devendra Singh Dhami, Kunapulli Gautam, Kristian Kersting, Sriraam Natarajan (<span class="puby">2019</span>): </span><span class="pubt">Fast Relational Probabilistic Inference and Learning: Approximate Counting via Hypergraphs.</span> <span class="pubv"> In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI).</span></div><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Fast Relational Probabilistic Inference and Learning: Approximate Counting via Hypergraphs</b><br><br>Counting the number of true instances of a clause is arguably a major bottleneck in relational
  probabilistic inference and learning. We approximate counts in two steps: (1) transform the fully
  grounded relational model to a large hypergraph, and partially-instantiated clauses to hypergraph
  motifs; (2) since the expected counts of the motifs are provably the clause counts, approximate
  them using summary statistics (in/out-degrees, edge counts, etc). Our experimental results
  demonstrate the efficiency of these approximations, which can be applied to many complex
  statistical relational models, and can be significantly faster than state-of-the-art,
  both for inference and learning, without sacrificing effectiveness.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="./../../papers/das2019aaai_couting.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Fast Relational Probabilistic Inference and Learning: Approximate Counting via Hypergraphs</b><br><br>@inproceedings{das2019aaai_couting,
  year = { 2019 },
  crossref = { https://starling.utdallas.edu/software/boostsrl/wiki/ },
  title = { Fast Relational Probabilistic Inference and Learning: Approximate Counting via Hypergraphs },
  pages = {  },
  booktitle = { Proceedings of the AAAI Conference on Artificial Intelligence (AAAI) },
  author = { Mayukh Das and Devendra Singh Dhami and Kunapulli Gautam and Kristian Kersting and Sriraam Natarajan }
}<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#ffa5c9;cursor: pointer;"> <a title="Code related to this paper" href="https://starling.utdallas.edu/software/boostsrl/wiki/" target="_blank"><font color="black">code</font></a>&nbsp;</span></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #090">Conference&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Approximate Counting,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Graph Databases,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Hypergraphs,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Lifted Inference</span></td></tr><tr role="row" class="odd"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/voelker2019ads.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><span class="mypub"><span class="puba">Claas Voelcker, Alejandro Molina, Johannes Neumann, Dirk Westermann, and Kristian Kersting (<span class="puby">2019</span>): </span> <span class="pubt">DeepNotebooks: Deep Probabilistic Models Construct Python Notebooks for Reporting Datasets. </span><span class="pubv">In Working Notes of the ECML PKDD 2019 Workshop on Automating Data Science (ADS).</span></span><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>DeepNotebooks: Deep Probabilistic Models Construct Python Notebooks for Reporting Datasets</b><br><br>Machine learning is taking an increasingly relevant role in
science, business, entertainment, and other fields. However, the most
advanced techniques are still in the hands of well-educated and -funded
experts only. To help to democratize machine learning, we propose Deep-Notebooks as a novel way to empower a broad spectrum of users, which
are not machine learning experts, but might have some basic programming
skills and are interested data science. Within the DeepNotebook
framework, users simply feed their datasets to the system. The system
then automatically estimates a deep but tractable probabilistic model
and then compiles an interactive Python notebook out of it that already
contains a preliminary yet comprehensive analysis of the dataset at hand.
If the users want to change the parameters of the interactive report or
make different queries to the underlying model, they can quickly do that
following the example code presented in the DeepNotebook. This
exibility allows the users to have a feedback loop where they can discover
patterns and dig deeper into the data using targeted questions, even if
they are not experts in machine learning.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="./../../papers/voelker2019ads.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>DeepNotebooks: Deep Probabilistic Models Construct Python Notebooks for Reporting Datasets</b><br><br>@incollection{voelker2019ads,
  year = { 2019 },
  crossref = { https://github.com/cvoelcker/DeepNotebooks },
  title = { DeepNotebooks: Deep Probabilistic Models Construct Python Notebooks for Reporting Datasets },
  pages = {  },
  booktitle = { Working Notes of the ECML PKDD 2019 Workshop on Automating Data Science (ADS) },
  author = { Claas Voelcker and Alejandro Molina and Johannes Neumann and Dirk Westermann and and Kristian Kersting }
}<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#ffa5c9;cursor: pointer;"> <a title="Code related to this paper" href="https://github.com/cvoelcker/DeepNotebooks" target="_blank"><font color="black">code</font></a>&nbsp;</span></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #1a8cff">Collection&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Automatic Statistician,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Data Reports,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Deep Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Density Estimtion,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Jupyter Notebook,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Shapley Explanation Values,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Sum-Product Networks</span></td></tr><tr role="row" class="even"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/ramanan2019tpm.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><span class="mypub"><span class="puba">Nandini Ramanan, Mayukh Das, Kristian Kersting, Sriraam Natarajan (<span class="puby">2019</span>): </span> <span class="pubt">Discriminative Non-Parametric Learning of Arithmetic Circuits. </span><span class="pubv">In Working Notes of the ICML 2019 Workshop on Tractable Probabilistic Models (TPM).</span></span><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Discriminative Non-Parametric Learning of Arithmetic Circuits</b><br><br>Arithmetic Circuits (AC) and Sum-Product Networks (SPN) have recently gained significant interest
by virtue of being tractable deep probabilistic models. Most previous work on learning
AC structures, however, hinges on inducing a tree-structured AC and, hence, may potentially
break loops that may exist in the true generative model. To repair such broken loops, we propose a
gradient-boosted method for structure learning of discriminative ACs (DACs), called DACBOOST.
Since, in discrete domains, ACs are essentially equivalent to mixtures of trees, DACBOOST decomposes
a large AC into smaller tree-structured ACs and learns them in a sequential, additive
manner. The resulting non-parametric manner of learning the DACs results in a model with very
few tuning parameters making our learned model significantly more efficient. We demonstrate on
standard data sets and some real-world data sets, the efficiency of DACBOOST compared to the
state-of-the-art DAC learners without sacrificing the effectiveness. This makes it possible to employ
DACs for large scale real-world tasks.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="./../../papers/ramanan2019tpm.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Discriminative Non-Parametric Learning of Arithmetic Circuits</b><br><br>@incollection{ramanan2019tpm,
  year = { 2019 },
  title = { Discriminative Non-Parametric Learning of Arithmetic Circuits },
  pages = {  },
  booktitle = { Working Notes of the ICML 2019 Workshop on Tractable Probabilistic Models (TPM) },
  author = { Nandini Ramanan and Mayukh Das and Kristian Kersting and Sriraam Natarajan }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #1a8cff">Collection&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Deep Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Gradient Boosting,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Structure Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Sum-Product Networks</span></td></tr><tr role="row" class="odd"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/shao2019tpm.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><span class="mypub"><span class="puba">Xiaoting Shao, Alejandro Molina, Antonio Vergari, Karl Stelzner, Robert Peharz, Thomas Liebig, Kristian Kersting (<span class="puby">2019</span>): </span> <span class="pubt">Conditional Sum-Product Networks: Imposing Structure on Deep Probabilistic Architectures. </span><span class="pubv">In Working Notes of the ICML 2019 Workshop on Tractable Probabilistic Models (TPM); also arXiv preprint arXiv:1905.08550.</span></span><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Conditional Sum-Product Networks: Imposing Structure on Deep Probabilistic Architectures</b><br><br>Bayesian networks are a central tool in machine learning and artificial intelligence, and make use
of conditional independencies to impose structure on joint distributions. However, they are generally
not as expressive as deep learning models and inference is hard and slow. In contrast, deep probabilistic
models such as sum-product networks (SPNs) capture joint distributions in a tractable fashion, but use
little interpretable structure. Here, we extend the notion of SPNs towards conditional distributions,
which combine simple conditional models into highdimensional ones. As shown in our experiments,
the resulting conditional SPNs can be naturally used to impose structure on deep probabilistic models,
allow for mixed data types, while maintaining fast and efficient inference.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="https://arxiv.org/pdf/1905.08550.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Conditional Sum-Product Networks: Imposing Structure on Deep Probabilistic Architectures</b><br><br>@incollection{shao2019tpm,
  year = { 2019 },
  title = { Conditional Sum-Product Networks: Imposing Structure on Deep Probabilistic Architectures },
  pages = {  },
  booktitle = { Working Notes of the ICML 2019 Workshop on Tractable Probabilistic Models (TPM); also arXiv preprint arXiv:1905.08550 },
  author = { Xiaoting Shao and Alejandro Molina and Antonio Vergari and Karl Stelzner and Robert Peharz and Thomas Liebig and Kristian Kersting }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #1a8cff">Collection&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Conditional Distribution,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Deep Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Gating Nodes,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Neural Conditionals,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Structure Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Sum-Product Networks</span></td></tr><tr role="row" class="even"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/kulessa2019query.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><span class="mypub"><span class="puba">Moritz Kulessa, Alejandro Molina, Carsten Binnig, Benjamin Hilprecht, Kristian Kersting (<span class="puby">2019</span>): </span> <span class="pubt">Model-based Approximate Query Processing. </span><span class="pubv">In Working Notes of the 1st International Workshop on Applied AI for Database Systems and Applications (AIDB) at VLDB 2019; also as arXiv preprint arXiv:1811.06224.</span></span><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Model-based Approximate Query Processing</b><br><br>Interactive visualizations are arguably the most important tool to explore, understand and convey facts about data. In the past years, the database community has been working on different techniques for Approximate Query Processing (AQP) that aim to deliver an approximate query result given a fixed time bound to support interactive visualizations better. However, classical AQP approaches suffer from various problems that limit the applicability to support the ad-hoc exploration of a new data set: (1) Classical AQP approaches that perform online sampling can support ad-hoc exploration queries but yield low quality if executed over rare subpopulations. (2) Classical AQP approaches that rely on offline sampling can use some form of biased sampling to mitigate these problems but require a priori knowledge of the workload, which is often not realistic if users want to explore a new database. In this paper, we present a new approach to AQP called Model-based AQP that leverages generative models learned over the complete database to answer SQL queries at interactive speeds. Different from classical AQP approaches, generative models allow us to compute responses to ad-hoc queries and deliver high-quality estimates also over rare subpopulations at the same time. In our experiments with real and synthetic data sets, we show that Model-based AQP can in many scenarios return more accurate results in a shorter runtime. Furthermore, we think that our techniques of using generative models presented in this paper can not only be used for AQP in databases but also has applications for other database problems including Query Optimization as well as Data Cleaning.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="https://arxiv.org/pdf/1811.06224.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Model-based Approximate Query Processing</b><br><br>@incollection{kulessa2019query,
  year = { 2019 },
  title = { Model-based Approximate Query Processing },
  booktitle = { Working Notes of the 1st International Workshop on Applied AI for Database Systems and Applications (AIDB) at VLDB 2019; also as arXiv preprint arXiv:1811.06224 },
  author = { Moritz Kulessa and Alejandro Molina and Carsten Binnig and Benjamin Hilprecht and Kristian Kersting }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #1a8cff">Collection&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8">  Databases,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Deep Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> SQL Queries,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Sum-Product Networks</span></td></tr><tr role="row" class="odd"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/schramowski2019arxiv_bert.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><span class="mypub"><span class="puba">Patrick Schramowski, Cigdem Turan, Sophie Jentzsch, Constantin Rothkopf, Kristian Kersting (<span class="puby">2019</span>): </span><span class="pubt">BERT has a Moral Compass: Improvements of ethical and moral values of machines. </span><span class="pubv">arXiv preprint arXiv:1912.05238 .</span></span><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>BERT has a Moral Compass: Improvements of ethical and moral values of machines</b><br><br>Allowing machines to choose whether to kill humans would be devastating for world peace and security.
    But how do we equip machines with the ability to learn ethical or even moral choices? Jentzsch et al.(2019) showed
    that applying machine learning to human texts can extract deontological ethical reasoning about "right" and "wrong"
    conduct by calculating a moral bias score on a sentence level using sentence embeddings. The machine learned that it is
    objectionable to kill living beings, but it is fine to kill time; It is essential to eat, yet one might not eat dirt; it
    is important to spread information, yet one should not spread misinformation. However, the evaluated moral bias was restricted
    to simple actions -- one verb -- and a ranking of actions with surrounding context. Recently BERT ---and variants such as RoBERTa
    and SBERT--- has set a new state-of-the-art performance for a wide range of NLP tasks. But has BERT also a better moral compass?
    In this paper, we discuss and show that this is indeed the case. Thus, recent improvements of language representations also improve
    the representation of the underlying ethical and moral values of the machine. We argue that through an advanced semantic representation
    of text, BERT allows one to get better insights of moral and ethical values implicitly represented in text. This enables the Moral
    Choice Machine (MCM) to extract more accurate imprints of moral choices and ethical values.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="Link to this paper" href="https://arxiv.org/abs/1912.05238" target="_blank"><font color="black">.url (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>BERT has a Moral Compass: Improvements of ethical and moral values of machines</b><br><br>@misc{schramowski2019arxiv_bert,
  year = { 2019 },
  title = { BERT has a Moral Compass: Improvements of ethical and moral values of machines },
  pages = {  },
  howpublished = { arXiv preprint arXiv:1912.05238 },
  author = { Patrick Schramowski and Cigdem Turan and Sophie Jentzsch and Constantin Rothkopf and Kristian Kersting }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #696969">Misc&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> BERT,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Contextual Embedding,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Deep Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Moral Machine,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Norms,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Social Bias</span></td></tr><tr role="row" class="even"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/kossen2019arxiv_stove.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><span class="mypub"><span class="puba">Jannik Kossen, Karl Stelzner, Marcel Hussing, Claas Voelcker, Kristian Kersting (<span class="puby">2019</span>): </span><span class="pubt">Structured Object-Aware Physics Prediction for Video Modeling and Planning. </span><span class="pubv">arXiv preprint arXiv:1910.02425 .</span></span><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>Structured Object-Aware Physics Prediction for Video Modeling and Planning</b><br><br>When humans observe a physical system, they can easily locate objects, understand their interactions, and anticipate
    future behavior, even in settings with complicated and previously unseen interactions. For computers, however, learning such
    models from videos in an unsupervised fashion is an unsolved research problem. In this paper, we present STOVE, a novel state-space
    model for videos, which explicitly reasons about objects and their positions, velocities, and interactions. It is constructed by
    combining an image model and a dynamics model in compositional manner and improves on previous work by reusing the dynamics model
    for inference, accelerating and regularizing training. STOVE predicts videos with convincing physical behavior over hundreds of
    timesteps, outperforms previous unsupervised models, and even approaches the performance of supervised baselines. We further demonstrate the strength of our model as a simulator for sample efficient model-based control in a task with heavily interacting objects
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="https://arxiv.org/pdf/1910.02425.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>Structured Object-Aware Physics Prediction for Video Modeling and Planning</b><br><br>@misc{kossen2019arxiv_stove,
  year = { 2019 },
  title = { Structured Object-Aware Physics Prediction for Video Modeling and Planning },
  pages = {  },
  howpublished = { arXiv preprint arXiv:1910.02425 },
  author = { Jannik Kossen and Karl Stelzner and Marcel Hussing and Claas Voelcker and Kristian Kersting }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #696969">Misc&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Attend-Infer-Repeat,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Deep Probabilistic Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Physical Interactions,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> SQAIR,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> SuPAIR,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Sum-Product Networks,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Video</span></td></tr><tr role="row" class="odd"><td><div class="mypubwrap"><div class="row"><div class="col-md-2"><img src="./../../images/hilprecht2019deepDB.png" class="pubimg" style="max-width: 110px; height: auto;vertical-align: left;"></div><div class="col-md-10"><span class="mypub"><span class="puba">Benjamin Hilprecht, Andreas Schmidt, Moritz Kulessa, Alejandro Molina, Kristian Kersting, Carsten Binnig (<span class="puby">2019</span>): </span><span class="pubt">DeepDB: Learn from Data, not from Queries!. </span><span class="pubv">arXiv preprint arXiv:1909.00607 .</span></span><div class="publ"><ul class="list-unstyled"><a style="color:black;font-size:12px;background-color:#ccccff" title="Abstract of this paper" href="#" class="abslink">&nbsp;abstract &nbsp;</a><div class="absinfo hidden"><a href="#" class="absclose" title="Close">x</a><br><br><b>DeepDB: Learn from Data, not from Queries!</b><br><br>The typical approach for learned DBMS components is to capture the behavior by running a representative
    set of queries and use the observations to train a machine learning model. This workload-driven approach, however,
    has two major downsides. First, collecting the training data can be very expensive, since all queries need to be
    executed on potentially large databases. Second, training data has to be recollected when the workload and the
    data changes. To overcome these limitations, we take a different route: we propose to learn a pure data-driven
    model that can be used for different tasks such as query answering or cardinality estimation. This data-driven
    model also supports ad-hoc queries and updates of the data without the need of full retraining when the workload
    or data changes. Indeed, one may now expect that this comes at a price of lower accuracy since workload-driven
    models can make use of more information. However, this is not the case. The results of our empirical evaluation
    demonstrate that our data-driven approach not only provides better accuracy than state-of-the-art learned components
    but also generalizes better to unseen queries.
<br><br><br></div>&nbsp;<span style="color:white;font-size:12px;background-color:#FFCECE;cursor: pointer;"> <a title="PDF of this paper" href="https://arxiv.org/pdf/1909.00607.pdf" target="_blank"><font color="black">&nbsp;.pdf (draft)</font></a>&nbsp;</span>&nbsp;<a style="color:black;font-size:12px;background-color:#FFCECE" title="This paper as BibTeX" href="#" class="biblink">&nbsp;.bib &nbsp;</a><div class="bibinfo hidden"><a href="#" class="bibclose" title="Close">x</a><br><br><b>DeepDB: Learn from Data, not from Queries!</b><br><br>@misc{hilprecht2019deepDB,
  year = { 2019 },
  title = { DeepDB: Learn from Data, not from Queries! },
  pages = {  },
  howpublished = { arXiv preprint arXiv:1909.00607 },
  author = { Benjamin Hilprecht and Andreas Schmidt and Moritz Kulessa and Alejandro Molina and Kristian Kersting and Carsten Binnig }
}<br><br><br></div></ul></div></div></div></div></td><td class="sorting_2" style="max-width: 120px; font-weight: normal; font-size: smaller; vertical-align: top;"><span style="color:white;font-size:11px;background-color: #696969">Misc&nbsp;</span><span style="color:black;font-size:11px;background-color: #E8E8E8"> Cardinality Estimation,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Databases,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Deep Probabilistic Learning,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Query Answering,</span>&nbsp;<span style="color:black;font-size:11px;background-color: #E8E8E8"> Sum-Product Networks</span></td></tr></tbody>
